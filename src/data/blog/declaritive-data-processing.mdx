---
title: "Declarative Data Engineering with Delta Tables: Treating Pipelines as Code"
author: eunsang
pubDatetime: 2025-12-27T00:00:00Z
modDatetime: 2025-12-27T00:00:00Z
slug: declarative-data-engineering-delta-tables-code-as-data
featured: false
draft: false
tags: ["spark", "delta", "data-engineering", "python", "decorators", "pipelines"]
description: ""
---

## So I Wanted to Treat Data Pipelines Like Code

There I was, staring at a Spark notebook, thinking: “I know how to write this pipeline imperatively—read CSV, clean, transform, write—but there has to be a cleaner way.”

Traditional pipelines work. But as datasets grow and transformations multiply, **imperative code becomes a spaghetti of reads, writes, and temporary views**. Debugging, refactoring, or adding new rules turns into a juggling act.

Then it clicked: why not treat **the pipeline itself as data**? If a program can inspect, transform, and execute code, why can’t a pipeline do the same with transformations and data quality rules?

Enter **code as data**—the old Lisp principle where programs are manipulable as structured data, not just lines to execute. If programs can be data, why not pipelines?

## Declarative Data Engineering: The Big Picture

In this notebook, we explore **declarative pipelines using Delta tables in a Marimo environment**, focusing on:

* Clear stages: Bronze → Silver → Gold
* Functional decorators to automate common patterns
* Self-documenting, reproducible analytics
* A practical example with transit train schedule data

Instead of manually managing Spark boilerplate, we let **decorators interpret “pipeline data”** and execute transformations automatically. You describe *what* should happen; the decorators handle *how* it happens.

## Exploring NYC Subway Stops

First, let’s visualize the **raw GTFS stops data**. Here’s the full unfiltered table rendered interactively:

<iframe
    src="https://kyrsang.github.io/blog-embed-01/"
    id="raw-stops-iframe"
    width="100%"
    height="600px"
    style="border:1px solid #ccc; overflow:hidden;"
    scrolling="no"
></iframe>

All stops are included, including parent and child stations.

## Filtering Stops Declaratively

We can apply a **functional filter** using our `expect_or_drop` decorator to only keep stops that have a `parent_station`. This is a **declarative approach**—we describe *what* to keep, not *how* to iterate and remove rows.

```python
@table("silver_trains")
@expect_all("train_id is not null", "departure_time is valid")
def transform_bronze_to_silver(bronze_df):
    return bronze_df
````

<iframe
    src="https://kyrsang.github.io/blog-embed-02/"
    id="filtered-stops-iframe"
    width="100%"
    height="600px"
    style="border:1px solid #ccc; overflow:hidden;"
    scrolling="no"
></iframe>

Only stops linked to a `parent_station` remain. Notice how the **pipeline itself is data**: the filtering logic is attached declaratively to the function, and the runtime executes it automatically.

Here, the function is **data**:

* The `@table` decorator knows how to persist it
* The `@expect_all` decorator knows what rules to apply
* The runtime interprets the decorated function and executes the transformations

We’re not just writing code—we’re **encoding a data pipeline as structured, executable data**, like Lisp treats nested lists as code and data.

## Functional Utility Decorators: Turning Pipeline Code into Data

Writing Spark pipelines is repetitive. Writing Delta tables, creating temp views, checking data quality—it’s a lot of copy-paste boilerplate.

We solved this by creating **decorators**: `@table`, `@temporary_view`, `@expect_or_drop`, `@expect_all`, `@expect_or_fail`. Here’s why they matter:

1. **Encapsulation of Boilerplate**
   `@table` writes a DataFrame to Delta with optional partitioning and schema evolution.

2. **Temporary Views Made Simple**
   `@temporary_view` registers a DataFrame as a Spark temporary view, making SQL queries easy.

3. **Declarative Data Quality Rules**
   Expectation decorators express rules cleanly:

   * `@expect_or_drop` drops rows failing a condition
   * `@expect_all` flags rows with `is_quarantined`
   * `@expect_or_fail` halts the pipeline if a critical rule fails

4. **Readable, Reusable Pipeline Code**
   The **pipeline itself becomes structured data**, with transformations, rules, and storage instructions interpretable by the runtime.

## Why This Feels Like Lisp for Data Engineering

* Lisp is **homoiconic**: code is represented as data structures programs can manipulate.
* Our **decorators turn pipeline functions into data structures** that the runtime can inspect, modify, and execute.
* Data quality rules, persistence paths, and transformation logic are **first-class metadata**—like S-expressions in Lisp.

Treating pipeline definitions as **data to be interpreted** allows us to serialize, reflect, and dynamically compose them in ways imperative pipelines rarely allow.

## Wrapping Up

Declarative pipelines aren’t just “nice-to-have.” They:

* Reduce boilerplate
* Improve readability
* Make data quality rules explicit
* Treat **pipeline code as structured, interpretable data**

By embracing **code-as-data principles**, Spark pipelines become **reproducible, maintainable, and self-documenting**.

Next time you write a pipeline, ask yourself: am I writing code that just executes, or am I encoding **data about how my data should flow**? Because if you do the latter, you’re doing **declarative data engineering with Delta tables** the right way.

<script>
    iFrameResize({ log: true }, '#filtered-stops-iframe');
</script>

<script>
    iFrameResize({ log: true }, '#raw-stops-iframe');
</script>
