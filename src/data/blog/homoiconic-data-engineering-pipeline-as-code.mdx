---
title: "Homoiconic Data Engineering: Why Your Pipeline Should Be Data"
author: eunsang
pubDatetime: 2025-12-27T00:00:00Z
modDatetime: 2025-12-27T00:00:00Z
slug: homoiconic-data-engineering-pipeline-as-code
featured: false
draft: false
tags: ["spark", "delta", "data-engineering", "python", "decorators", "pipelines"]
description: ""
---

## The Zen of Declarative Data: Moving from "Steps" to "Specs"

If you look at a typical Spark notebook, you usually see a story of verbs: *Read* this, *filter* that, *join* these, *write* there. This is imperative programming—a manual recipe for data.

But there is a more powerful way to think about data engineering: Data as Code. Instead of writing a sequence of commands, we define our data structures in a way that the system can "read" and execute for us. We stop writing recipes and start writing specifications.

### The Problem: The "Opaque" Pipeline

In an imperative pipeline, logic is trapped. If you want to know what the data quality rules are, you have to read every line of the script. It’s a "black box"—you can't easily extract the logic to generate a report or a lineage map because the logic is inextricably tied to the execution.

By treating our Pipeline as Code, we make the logic visible, reusable, and structured.

### Exploring the Raw Reality

Before we can define what the data *should* be, we have to see what it *is*. Here is our raw GTFS transit data for NYC subway stops—unfiltered and messy:

<iframe
src="https://kyrsang.github.io/blog-embed-01/"
width="100%"
height="560px"
style={{
borderRadius: "12px",
boxShadow: "0 4px 12px rgba(0, 0, 0, 0.1)",
overflow: "hidden"
}}
scrolling="no"
/>

### Declarative Engineering: Encoding Intent

In a declarative system, we treat our transformations as metadata. By using Python decorators, we wrap our logic in a layer of "Data as Code."

```python
@table("silver_trains")
@expect_or_drop(lambda df: df["parent_station"].notna())
def transform_bronze_to_silver(bronze_df):
    return bronze_df

```

### Why this is "Data as Code" in Principle

In DaC, we treat an arbitrary structure (like a function or a list of rules) as something that can be exposed via a specialized API. Here, the function itself is the "data" that our decorators consume:

1. The Function is a Data Point: The `@table` decorator "reads" your function. It knows the destination is `silver_trains` before a single row of data even moves.
2. Logic as Metadata: The `@expect_or_drop` decorator isn't just a filter; it’s <u>executable documentation</u>. We’ve turned a business rule into a data attribute of the pipeline itself.

The result is a refined dataset where the rules are applied automatically by the runtime:

<iframe
src="https://kyrsang.github.io/blog-embed-02/"
width="100%"
height="560px"
style={{
borderRadius: "12px",
boxShadow: "0 4px 12px rgba(0, 0, 0, 0.1)",
overflow: "hidden"
}}
scrolling="no"
/>

### Deep Thought: The Power of Introspection

The real "Data as Code" magic happens when you realize that because your pipeline is now structured data, your system can introspect it.

In a traditional script, the computer doesn't know what the script *means* until it runs it. In this declarative model, the decorators create a registry. You can write a small utility to "query" your code:

* *"Show me all tables that have a null-check on `parent_station`."*
* *"List all tables written to the 'Silver' layer."*

This turns your codebase into a searchable catalog of logic.

### Clean Code Principle: The Open-Closed Pipeline

This approach honors the **Open-Closed Principle** (from SOLID): software entities should be open for extension but closed for modification.

When you need to add a new data quality rule, you don't go into the "guts" of the Spark transformation and risk breaking the join logic. You simply add a new decorator—a new piece of "data" to the function's definition. You are extending the pipeline's behavior without modifying its core execution logic.

### The Power of the Declarative Approach

By turning our pipeline logic into "data" that our decorators can interpret, we gain three major advantages:

* Self-Documenting Pipelines: The decorators act as a "manifest," making the storage path and constraints obvious at a glance.
* Separation of Concerns: The "how" (Delta Lake optimization, schema evolution) is decoupled from the "what" (the business transformation).
* Pipeline Intelligence: We can programmatically generate lineage and audit logs because our "code" is now a structured data object.

### Conclusion: From Coder to Architect

When we treat Data as Code, we stop being people who move bytes from point A to point B. We become architects of a system that understands its own rules.

Using Delta Tables and declarative decorators allows us to build pipelines that aren't just a series of commands, but a robust, searchable, and interpretable map of our data’s journey.
