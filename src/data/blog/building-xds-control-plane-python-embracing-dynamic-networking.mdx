---
title: "Building an xDS Control Plane in Python: Embracing Dynamic Networking"
author: eunsang
pubDatetime: 2025-08-03T10:00:00Z
modDatetime: 2025-08-03T10:00:00Z
slug: building-xds-control-plane-python-embracing-dynamic-networking
featured: false
draft: false
tags: ["envoy", "xds", "grpc", "python", "network", "buf", "uv"]
description: ""
--- 

## Decoding the Envoy Control Plane: An xDS Overview

So there I was, trying to teach Envoy how to think.

The idea is solid: instead of hardcoding your proxy config in static YAML, you spin up a separate control plane that tells Envoy what to do — live, over gRPC.

The xDS API family is extensive, and it can take a bit of time to understand what each component does. Here's a breakdown of the key members:

* **LDS (Listener Discovery Service)** – Configures which ports Envoy should listen on and how to process incoming traffic.
* **CDS (Cluster Discovery Service)** – Defines clusters, representing the upstream services Envoy can connect to.
* **RDS (Route Discovery Service)** – Specifies routing rules, such as directing requests for `/users` to the user service.
* **EDS (Endpoint Discovery Service)** – Provides the actual IP addresses of healthy instances within a cluster.

And here’s the catch: these configs need to arrive in the right order. You can’t route to a cluster that doesn’t exist. You can’t define an endpoint before the cluster is known. Enter *ADS* — Aggregated Discovery Service.

Instead of juggling four separate gRPC streams (one per service), ADS gives you a single, unified stream where everything flows through one channel. CDS → EDS → RDS, in perfect order. Envoy connects once, and the control plane sequences the updates.

## SotW ADS in Action

Envoy’s docs spell out four xDS transport variants along two axes—State-of-the-World vs. incremental, and per-type vs. aggregated streams:

> * **State of the World (Basic xDS):** SotW, separate gRPC stream for each resource type
> * **Incremental xDS:** incremental, separate gRPC stream for each resource type
> * **Aggregated Discovery Service (ADS):** SotW, aggregate stream for all resource types
> * **Incremental ADS:** incremental, aggregate stream for all resource types

Our Python control plane sits in the **Aggregated Discovery Service** (ADS) + **State-of-the-World** corner:

* **Single gRPC stream for everything.** Envoy calls only `StreamAggregatedResources` on our `AggregatedDiscoveryServiceServicer`, and carries LDS, CDS, RDS, EDS, etc., as logical sub-streams over that one channel.
* **Full snapshots on every update.** Whenever Envoy’s `version_info` is empty, doesn’t match our server version, or it NACKs, we bundle up **all** of the requested resources of that type and send them together—no delta logic, no piecemeal updates.
* **Guaranteed ordering.** By delivering Listeners → Clusters → Routes → Endpoints on the same wire, we sidestep “I got my routes before my cluster existed” errors.

If you later need “send-only-what-changed” behavior or lazy loading at scale, you can switch to **Incremental ADS** (`DeltaAggregatedResources`). But for a lean, easy-to-debug demo, SotW ADS gives us exactly the right mix of simplicity and correctness.

## First Contact: How Envoy Finds Its Control Plane

Before our Python server can work its magic, the Envoy proxy needs to be told one simple, critical thing: where to find the xDS server. This is the sole job of the static `envoy.yaml` file. It’s the "bootstrap" configuration that gets the proxy’s lights on and points it in the right direction.

My `envoy.yaml` shows this perfectly. It’s mostly static housekeeping, but the key sections bridge the static world with our dynamic one.

* **The Control Plane Address:** The `dynamic_resources` section is where the magic begins. It tells Envoy that all its configuration—Listeners, Clusters, Routes—will come from a dynamic source via the *Aggregated Discovery Service* (`ads_config`). Critically, it points to a *static cluster* named `xds_cluster` for this purpose.
* **A Static Cluster for a Dynamic World:** This `xds_cluster` is a bit of an anachronism—a static cluster defined in a file—but it’s essential. Its only purpose is to define a host (`xds-server`) and a port (`5678`) so Envoy knows how to establish the initial gRPC connection to our Python server.

Think of it this way: our `envoy.yaml` isn’t the destination. It’s the ignition key and map to the real destination—our Python xDS server.

## From Listener to Cluster: Rebuilding Envoy’s L4–L7 Request Path in Python

Building this server wasn’t just about writing code—it was about *thinking like a request*. We traced a single HTTP request’s entire path through Envoy, from the moment it hits the proxy to when it reaches the backend. Each stop along the way maps to a Protobuf message we had to dig up in Envoy’s API docs and then bring to life in Python.

Our `xds_server.py` is basically a step-by-step reenactment of this flow. Here’s how the pieces connect:

#### The Downstream Doorway: The Listener (L4)

It all kicks off in `generate_listener_config()`. This function creates a **Listener**—think of it as Envoy’s front door—listening on port `15001`. But this listener isn’t just an endpoint; it starts a *filter chain*, which is Envoy’s way of processing connections step by step.

The first and most critical filter in this chain is the **HttpConnectionManager**—an L4 network filter that upgrades raw TCP streams into something meaningful: HTTP requests.

#### The Traffic Director: The HTTP Connection Manager (L7)

When a TCP connection arrives, the `HttpConnectionManager` takes control. It’s the brains that translate raw network bytes into structured HTTP messages—requests and responses that Envoy understands and can manipulate.

In our Python code, this configured `HttpConnectionManager` is wrapped inside a generic Protobuf `Any` message—because xDS streams can carry many resource types—and attached directly to the Listener’s filter chain.

#### The Rulebook: Routing and HTTP Filters

Once the `HttpConnectionManager` has parsed the HTTP request, it needs instructions on where to send it next. That’s where **RouteConfiguration** comes in.

We keep it simple: any request matching the path prefix `/` gets routed to the `httpbin_service` cluster. This rulebook is the core of Envoy’s routing logic.

Next, the request is pushed through L7 HTTP filters. We only include the essential one: `envoy.filters.http.router`. This filter’s job is to finalize the routing decision, pick the right upstream cluster based on the RouteConfiguration, and actually send the request out.

#### The Final Destination: The Upstream Cluster

With the route chosen, the request leaves Envoy through a **Cluster**. Our `generate_cluster_config()` sets up the `httpbin_service` cluster. It tells Envoy to resolve the hostname `httpbin` (via Docker’s DNS, thanks to `LOGICAL_DNS` discovery), and balance traffic across its endpoints using `ROUND_ROBIN` load balancing—even if it’s just one backend container listening on port `8000`.

From opening port 15001 to forwarding requests upstream, the entire flow lives in a few hundred lines of Python. Each part we configure is a Protobuf message, streamed by our `XdsServer` over gRPC, handing Envoy a dynamic, fully detailed playbook—no static YAML needed.

## Conclusion

Getting an xDS server *up and running* is one thing — but truly understanding the path to that point is a whole different story. This project felt like a microcosm of modern microservice challenges: complex, sometimes maddening, but rewarding.

What really stuck with me is that this isn’t about static files anymore. Static YAML is simple to read, but brittle and slow to change. xDS flips the script: it’s a living, breathing system — a continuous control loop that adapts on the fly.

Our Python server is a mini version of that philosophy. It’s not a static snapshot; it’s an active, intelligent agent streaming updates, ready to evolve as the network changes.

This isn’t just about building a functional server—it’s about adopting a new mindset: treating dynamic networking as code. The future isn’t defined by flawless YAML files, but by systems that communicate, adapt, and reconfigure themselves in real time.
