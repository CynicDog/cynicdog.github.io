---
title: "Building an xDS Control Plane in Python: Embracing Dynamic Networking"
author: eunsang
pubDatetime: 2025-08-03T10:00:00Z
modDatetime: 2025-08-03T10:00:00Z
slug: building-xds-control-plane-python-embracing-dynamic-networking
featured: false
draft: false
tags: ["envoy", "xds", "grpc", "python", "network", "buf", "uv"]
description: ""
--- 

## Decoding the Envoy Control Plane: An xDS Overview

So there I was, trying to teach Envoy how to think.

Well. Let’s just say you don’t fully appreciate the chaos of modern networking until you meet xDS — Envoy’s Extensible Discovery Service.

The idea is solid: instead of hardcoding your proxy config in static YAML, you spin up a separate control plane that tells Envoy what to do — live, over gRPC. Need to add a new service? Change a route? Rotate a backend IP? Push it via xDS. No restarts. No redeploys. No drama.

But the xDS family is big — and it takes some time to figure out who’s doing what.

* **LDS** tells Envoy what ports to listen on and how to handle incoming requests.
* **CDS** defines the clusters — i.e. upstream services you want to talk to.
* **RDS** wires up routing rules like “send `/users` to the user-service.”
* **EDS** fills in the actual IPs of healthy service instances.

And here’s the catch: these configs need to arrive in the right order. You can’t route to a cluster that doesn’t exist. You can’t define an endpoint before the cluster is known. Enter *ADS* — Aggregated Discovery Service.

Instead of juggling four separate gRPC streams (one per service), ADS gives you a single, unified stream where everything flows through one channel. CDS → EDS → RDS, in perfect order. Envoy connects once, and the control plane sequences the updates.

> It’s not just cleaner — it’s essential. Without ADS, you’d need to sync multiple streams across different servers, and pray your updates land in the right order. ADS eliminates that risk by design.

## SotW ADS in Action

Envoy’s docs spell out four xDS transport variants along two axes—State-of-the-World vs. incremental, and per-type vs. aggregated streams:

> * **State of the World (Basic xDS):** SotW, separate gRPC stream for each resource type
> * **Incremental xDS:** incremental, separate gRPC stream for each resource type
> * **Aggregated Discovery Service (ADS):** SotW, aggregate stream for all resource types
> * **Incremental ADS:** incremental, aggregate stream for all resource types

Our Python control plane sits in the **Aggregated Discovery Service** (ADS) + **State-of-the-World** corner:

* **Single gRPC stream for everything.** Envoy calls only `StreamAggregatedResources` on our `AggregatedDiscoveryServiceServicer`, and carries LDS, CDS, RDS, EDS, etc., as logical sub-streams over that one channel.
* **Full snapshots on every update.** Whenever Envoy’s `version_info` is empty, doesn’t match our server version, or it NACKs, we bundle up **all** of the requested resources of that type and send them together—no delta logic, no piecemeal updates.
* **Guaranteed ordering.** By delivering Listeners → Clusters → Routes → Endpoints on the same wire, we sidestep “I got my routes before my cluster existed” errors.

If you later need “send-only-what-changed” behavior or lazy loading at scale, you can switch to **Incremental ADS** (`DeltaAggregatedResources`). But for a lean, easy-to-debug demo, SotW ADS gives us exactly the right mix of simplicity and correctness.

## The Missing Link: Envoy’s Bootstrap (`envoy.yaml`)

Before our Python server can work its magic, the Envoy proxy needs to be told one simple, critical thing: where to find the xDS server. This is the sole job of the static `envoy.yaml` file. It’s the "bootstrap" configuration that gets the proxy’s lights on and points it in the right direction.

My `envoy.yaml` shows this perfectly. It’s mostly static housekeeping, but the key sections bridge the static world with our dynamic one.

* **The Control Plane Address:** The `dynamic_resources` section is where the magic begins. It tells Envoy that all its configuration—Listeners, Clusters, Routes—will come from a dynamic source via the *Aggregated Discovery Service* (`ads_config`). Critically, it points to a *static cluster* named `xds_cluster` for this purpose.
* **A Static Cluster for a Dynamic World:** This `xds_cluster` is a bit of an anachronism—a static cluster defined in a file—but it’s essential. Its only purpose is to define a host (`xds-server`) and a port (`5678`) so Envoy knows how to establish the initial gRPC connection to our Python server.
* **Avoiding Warnings:** The rest of the file is a testament to how configuration evolves. The `admin` and `overload_manager` sections aren’t for routing traffic; they set up the proxy itself. The new, modern configs for these sections were crucial to eliminate the deprecation warnings that plagued my initial attempts—a classic example of how a static file can cause headaches if not kept up-to-date.

Think of it this way: our `envoy.yaml` isn’t the destination. It’s the ignition key and map to the real destination—our Python xDS server.

## From Listener to Cluster: Rebuilding Envoy’s L4–L7 Request Path in Python

Building this server wasn’t just about writing code—it was about *thinking like a request*. We traced a single HTTP request’s entire path through Envoy, from the moment it hits the proxy to when it reaches the backend. Each stop along the way maps to a Protobuf message we had to dig up in Envoy’s API docs and then bring to life in Python.

Our `xds_server.py` is basically a step-by-step reenactment of this flow. Here’s how the pieces connect:

#### The Downstream Doorway: The Listener (L4)

It all kicks off in `generate_listener_config()`. This function creates a **Listener**—think of it as Envoy’s front door—listening on port `15001`. But this listener isn’t just an endpoint; it starts a *filter chain*, which is Envoy’s way of processing connections step by step.

The first and most critical filter in this chain is the **HttpConnectionManager**—an L4 network filter that upgrades raw TCP streams into something meaningful: HTTP requests.

#### The Traffic Director: The HTTP Connection Manager (L7)

When a TCP connection arrives, the `HttpConnectionManager` takes control. It’s the brains that translate raw network bytes into structured HTTP messages—requests and responses that Envoy understands and can manipulate.

In our Python code, this configured `HttpConnectionManager` is wrapped inside a generic Protobuf `Any` message—because xDS streams can carry many resource types—and attached directly to the Listener’s filter chain.

#### The Rulebook: Routing and HTTP Filters

Once the `HttpConnectionManager` has parsed the HTTP request, it needs instructions on where to send it next. That’s where **RouteConfiguration** comes in.

We keep it simple: any request matching the path prefix `/` gets routed to the `httpbin_service` cluster. This rulebook is the core of Envoy’s routing logic.

Next, the request is pushed through L7 HTTP filters. We only include the essential one: `envoy.filters.http.router`. This filter’s job is to finalize the routing decision, pick the right upstream cluster based on the RouteConfiguration, and actually send the request out.

#### The Final Destination: The Upstream Cluster

With the route chosen, the request leaves Envoy through a **Cluster**. Our `generate_cluster_config()` sets up the `httpbin_service` cluster. It tells Envoy to resolve the hostname `httpbin` (via Docker’s DNS, thanks to `LOGICAL_DNS` discovery), and balance traffic across its endpoints using `ROUND_ROBIN` load balancing—even if it’s just one backend container listening on port `8000`.

From opening port 15001 to forwarding requests upstream, the entire flow lives in a few hundred lines of Python. Each part we configure is a Protobuf message, streamed by our `XdsServer` over gRPC, handing Envoy a dynamic, fully detailed playbook—no static YAML needed.

## Conclusion

Getting an xDS server *up and running* is one thing — but truly understanding the path to that point is a whole different story. This project felt like a microcosm of modern microservice challenges: complex, sometimes maddening, but rewarding.

What really stuck with me is that this isn’t about static files anymore. Static YAML is simple to read, but brittle and slow to change. xDS flips the script: it’s a living, breathing system — a continuous control loop that adapts on the fly.

Our Python server is a mini version of that philosophy. It’s not a static snapshot; it’s an active, intelligent agent streaming updates, ready to evolve as the network changes.

This isn’t just about building a working server; it’s about embracing a new mindset: dynamic networking as code.

The future isn’t perfect YAML files. It’s systems that talk, listen, and reconfigure themselves.
